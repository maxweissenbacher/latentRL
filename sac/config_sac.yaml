# environment and task
env:
  nu: 0.05
  num_actuators: 5
  num_sensors: 256
  actuator_scale: 0.1
  num_envs: 5  # Number of parallel training envs
  burnin: 0
  target:
  exp_name: KS
  path_to_cae_model:  # if empty or None, no trained autoencoder will be used
  frame_skip: 1  # Sticky actions
  soft_action: True  # Interpolate the actions over frame_skip frames
  seed: 2253461632  # keep fixed for now
  auto_generate_seed: True  # If True, generate a random seed. If False, use the seed value specified above.

# collector
collector:
  total_frames: 500_000
  init_random_frames: 0
  frames_per_batch: 1000
  device: cpu
  reset_at_each_iter: False

# replay buffer
replay_buffer:
  size: 1_000_000
  prb: False # use prioritized experience replay; the original TQC paper does not prioritized replay
  scratch_dir:

# optim
optim:
  utd_ratio: 1.0
  actuator_loss_weight: 0.0  # Weight hyperparameter of actuation size in reward function
  gamma: 0.9
  lr: 3.0e-4
  weight_decay: 0.0  # L2 regularisation | Works well for LSTM | For attention weight_decay > 0 leads to NaNs
  batch_size: 2048
  target_update_polyak: 0.995
  alpha_init: 1.0
  adam_eps: 1.0e-8

# network
network:
  actor_hidden_sizes: [256, 256]
  critic_hidden_sizes: [512, 512, 512]
  sac_critic_hidden_sizes: [256, 256]
  activation: relu  # The original TQC paper uses ReLu
  default_policy_scale: 1.0
  scale_lb: 0.1
  device: cpu
  auto_detect_device: True  # If True, will use GPU when available or else fall back on device specified above

# logging
logger:
  backend: wandb
  mode: online
  eval_iter: 10  # Evaluate every x batches
  test_episode_length: 1_000
  project_name: LatentRL
  team_name: why_are_all_the_good_names_taken_aaa

# Disable output subdirectory
hydra:
  output_subdir: config
